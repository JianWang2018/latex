\chapter{Ensemble Methods for Prediction}\label{ch:ensemble}
AdaBoost and random forest learning algorithms are two machine learning models that we use to improve  prediction performance of price change for limit order books. These models are two widely used ensemble methods. In this chapter,  we give a brief description of ensemble methods, especially AdaBoost and random forest algorithms.

\section{An introduction to ensemble methods}
Ensemble methods are designed for improving the performance of a statistical fitting model. It combines the results of some fitting models,  if those models are relatively independent,  the combined result will decrease the predicting errors. 
To be formal,   let's consider a real-valued function 
\begin{equation}
	g:\mathbb{R}^d\rightarrow \mathbb{R}
\end{equation}

that is based on data sample $(X_1, Y_1), ..., (X_n, Y_n)$,  where $X$ is in a $d$-dimensional real value space and $Y$ is the response variable based on $X$. We define a projecting process called ``\textit{base procedure}'' which will generate a real-value function based on some $d$-dimensional real input data. To use ensemble methods,  we usually do the base procedure several times by changing the input data sample. For example ,  we can give different weights to dependent variables in each procedure to get different predicting results, ${g_1(\hat{\omega}_1^T X)}, {g_2(\hat{\omega}_2^T X)}, {g_3(\hat{\omega}_3^T X)}, ..., {g_n(\hat{\omega}_n^T X)}$,  where $\hat{\omega}_i$ is the weights in the model in procedure $i$ and $n$ is the number of times that we run the model. Therefore,  the results of the ensemble methods $\hat{g}_{ens}X$ is the linear combinations of the generated individual function ${g_k(\hat{\omega}_k^T X)}$
\begin{equation}
\hat{g}_{ens}(X)=\sum_{k=1}^{n}c_k\hat{g}_k(\omega_k^T X)
\end{equation}
where $C_k$ is the coefficients of a linear combination of the individual function $\hat{g}_k$.The $c_k$ is usually identical,  which equal to $\frac{1}{n}$ for the linear model. For other models, such as boosting,  $\sum_{k-1}^{n}c_k$ will increase when n gets larger.

Why the ensemble models will improve the predictive performance of an individual model? The reasons can be concluded in the following: 1) Re-sampling the data, such as bagging procedure, can be treated as a variance reduction method.\cite{buhlmann2012bagging} 2) Boosting methods can reduce both bias and variance. Since boosting is a weighted average of some weaker classifiers,  so it usually has lower variance. Besides,  boosting method pays more attention to the sample of poor predictions,  so it often reduces the bias too.  
\section{Bagging}
Bagging is a very important ensemble method. Since we will both use bagging method during the process of building AdaBoost and random forest scheme,  so in this section, we give a short introduction of this technique.
Suppose we have pairs $(X_i, Y_i),  i=1, ..., n$ ,  where $X_i$ is a d-dimensional real value vector and $Y_i$ is a real value defined as a response variable.  If $Y_i$ only takes discrete integers,  then it is a classification problem,  otherwise,  it is a regression problem. Now define a function estimator,  which generates result via a given base process \:
\begin{equation}
\hat{g}(\cdot)=h_n((X_1, Y_1), ..., (X_n, Y_n))(\cdot):\mathcal{R}^d\rightarrow \mathcal{R}
\end{equation}  
where $h_n$ is a function that estimates a function according to the given data. 
The algorithm of bagging is in the following:\\
\\
\begin{algorithm}[H]
	\caption{Bagging algorithm, \cite{breiman1996bagging}}\label{alg:bagging}
	\nl Randomly draw n times with replacement from the data $(X_1, Y_1), ..., (X_n, Y_n)$ and define a bootstrap sample data as $(X_1^*, Y_1^*), ..., (X_n^*, Y_n^*)$\;
	\nl Derive the bootstrapped estimator $\hat{g}^*(\cdot)$,  $\hat{g}^*(\cdot)=h_n((X_1^*, Y_1^*), ..., (X_n^*, Y_n^*))(\cdot)$\;
	\nl Repeat step 1 and step 2 $M$ times,  where $M$ is often chosen as 50 or 100,  yielding $\hat{g}^{*k}(\cdot) (k=1, ..., M)$. The bagged estimator is $\hat{g}_{Bag}(\cdot)=M^{-1}\sum_{k=1}^{M}\hat{g}^{*k}(\cdot)$	
\end{algorithm}

\section{Bootstrap}
Bootstrap is another important scheme that is used in random forest method. It was first introduced by  
\cite{efron1979bootstrap}. This algorithm draws plenty of separate samples of bootstrap scheme, analyzes those replications which are generated by  bootstrap process, and calculate the empirical standard error to evaluates the standard error of parameter $\hat{\theta}$ , denoted by $\hat{se}_B$. In $\hat{se}_B$, $B$ represents the number of bootstrap samples that we used. The description of algorithm is described in the following:\\

\begin{algorithm}[H]
	\caption{Bootstrap algorithm, \cite{breiman1996bagging}}\label{alg:bootstrap}
	\nl Choose $B$ independent bootstrap samples $x^*_1,...,x^*_B$, every sample contains $n$ data values drawing with replacement from $x$.\\
	\nl Estimates the bootstrap replication corresponding to each bootstrap sample\\
	$$\hat{\theta}^*(b)=s(x_b^*),b=1,...,B$$\\
	\nl Evaluate the standard error $se_F(\hat{\theta})$ by the sample standard error of the $B$ replicates\\
	
	$$\hat{se}_B=[\frac{1}{B-1}\sum_{b=1}^{B}(\hat{\theta}^*(b)-\hat{\theta}^*(.))^2]$$\\
	with \\
	$$\hat{\theta}^*(.)=B^{-1}\sum_{b=1}^{B}\hat{\theta}^*(b)$$
\end{algorithm}

When the sample size $B$ goes to infinity, the limit of $\hat{se}_B$ becomes an ideal bootstrap estimate.



\section{AdaBoost learning scheme}
This section we study the procedure and properties of an important learning scheme,  AdaBoost,  which is deemed as ``the best off the shelf classifier" in the word(\cite{friedman2000additive}). The purpose of boosting method is to combine many weaker models together to generate a strong ``committee".
\cite{freund1995desicion} first introduced the AdaBoost scheme. Suppose we have a response variable $Y$,  which can only take two values,  1 and -1.  We also assume a vector of independent variables $X$ and the predicted result of a classifier $G(x)$ can also only take one of the two values ${1, -1}$. The error rate of the predict value $G(x)$ and the true value is:
\begin{equation}
\bar{err}=\frac{1}{N}\sum_{i=1}^{N}I(y_i\neq G(x_i))
\end{equation}

Now we assume that $G_m(x)$ is a sequence of weak classifiers,  $m=1, 2, ..., M$, the strong classifier $G(x)$ is obtained through a weighted majority vote of the results generated by the weak classifiers $G_m(x)$:
\begin{equation}
G(x)=sign(\sum_{m=1}^{M}\alpha_m G_m(x))
\end{equation}

where $\alpha_1, \alpha_2, ..., \alpha_M$ are generated by the AdaBoost algorithm. In each process,  weights will be updated and those data that are misclassified will get more attention. Algorithm \ref{alg:AdaBoost} shows the main procedure of AdaBoost.
At beginning,  all the initial weights are set to be $\frac{1}{N}$,  which means that they are all equal. For each successive step,  the weights are individually changed based on the classification result on the last step. For example,  at step k,  those data that are misclassified by the classifier $G_{k-1}$ will increase their weights, meanwhile, those data that are correctly classified will decrease their weights.The factor that we update the weights is $exp(\alpha_m)$ in the algorithm,  which is an exponential function of the error rate in the last step.\\
\\
\begin{algorithm}[H]
	\caption{AdaBoost algorithm,  \cite{friedman2001elements}}\label{alg:AdaBoost}
	\nl Initialize the observation weights $\omega_i$=$1/N$, $i$=1, 2, ..., $N$\;
	\nl \For{m=1 to M}{
		\nl Fit a classifier $G_m(x)$ to the training data using weights $\omega_i$\;
		\nl   Compute \;
		\begin{equation*}		      
		err_m=\frac{\sum_{i=1}^{N}\omega_i I(y_i \neq G_m(x_i))}{\sum_{i=1}^{N}\omega_i}		       
		\end{equation*} 
		\nl Compute $\alpha_m=log((1-err_m)/err_m)$\;
		\nl Set $\omega_i \gets \omega_i \cdot exp[\alpha_m \cdot I(y_i \neq G_m(x_i))]$,  $i =1, 2, ..., N$ \;
	}
	\nl Output $G(x)=sign[\sum_{m=1}^{M}\alpha_m G_m(x)]$ \;
\end{algorithm}
In general,  AdaBoost is a kind of forward stag-wise additive model that uses the exponential loss function. Algorithm \ref{alg:forward} shows the procedure of forward stag-wise additive model. \\
\\
\begin{algorithm}[H]
	\caption{Forward Stag-wise Additive Modeling,  \cite{friedman2001elements}}\label{alg:forward}\label{alg:forward}
	\nl Initialize $f_0(x)=0$\;
	\nl \For{m=1 to M}{
		 a) Compute \;
		$(\beta_m, \gamma_m)=\underset{\beta, \gamma}{\operatorname{argmin}}\sum_{i=1}^{N}L(y_i, f_{m-1}(x_i)+\beta b(x_i;\gamma))$ \;
		 b) Set $f_m(x)=f_{m-1}(x)+\beta_m b(x;\gamma_m)$
	}
\end{algorithm}

The loss function of AdaBoost is :
\begin{equation}\label{en:loss}
L(y, f(x))=exp(-yf(x))
\end{equation}

Substitute into algorithm \ref{alg:forward},  we can get
\begin{equation}
(\beta_m, \gamma_m)=\underset{\beta, G}{\operatorname{argmin}}\sum_{i=1}^{N}exp(-y_i(f_{m-1}(x_i)+\beta G(x_i)) 
\end{equation}
This can be transformed to:
\begin{equation}\label{en:loss_exp}
(\beta_m, \gamma_m)=\underset{\beta, G}{\operatorname{argmin}}\sum_{i=1}^{N}\omega_i^{(m)}exp(-\beta y_i G(x_i)) 
\end{equation}

where $\omega_i^{(m)}=exp(-y_if_{m-1}(x_i))$. To solve equation \ref {en:loss_exp},  first,  assume $\beta>0$,  the solution of $G_m(x)$ in equation \ref{en:loss_exp} is:
\begin{equation}
G_m=\underset{\beta, G}{\operatorname{argmin}}\sum_{i=1}^{N}\omega_i^{(m)}I(y_i\neq G(x_i)) 
\end{equation} 
To prove this,  from equation \ref{en:loss_exp},  we can easily get:
\begin{equation}
\sum_{i=1}^{N}\omega_i^{(m)}exp(-\beta y_i G(x_i))=e^{-\beta}\sum_{y_i=G(x_i)}\omega_i^{(m)}+e^{\beta}\sum_{y_i\neq G(x_i)}\omega_i^{(m)}
\end{equation}
which can be transformed into the following form if we add and minus term $e^{-\beta} \sum_{y_i \neq G(x_i)}\omega_i^{(m)}$ simultaneously.
\begin{equation}
(e^\beta-e^{-\beta})\sum_{i=1}^{N}\omega_i^{(m)}I(y_i \neq G(x_i))+e^{-\beta}\sum_{i=1}^{N}\omega_i^{(m)}
\end{equation}
Now put $G_m$ into equation \ref{en:loss_exp} and solve parameter $\beta$ by taking first derivative,  we can get:
\begin{equation}
\beta_m=\frac{1}{2}log\frac{1-err_m}{err_m}
\end{equation}
where $err_m$ is the weighted error rate that is minimized:
\begin{equation}
err_m=\frac{\sum_{i=1}^{N}\omega_i^{(m)}I(y_i\neq G_m(x_i))}{\sum_{i=1}^{N}\omega_i^{(m)}}
\end{equation}
Now the function $f_m(x)$ is updated:
\begin{equation}
f_m(x)=f_{m-1}(x)+\beta_mG_m(x)
\end{equation}
and the weight in the next step is:
\begin{equation}\label {en:weight_update}
\omega^{(m+1)}=\omega_i^{(m)}\cdot e^{-\beta_my_iG_m(x_i)}
\end{equation}
since  $\omega_i^{(m)}=exp(-y_if_{m-1}(x_i))$\\
We know that $-y_iG_m(x_i)=2\cdot I(y_i\neq G_m(x_i))-1$,  so equation \ref{en:weight_update} can be written as :
\begin{equation}\label{en:weight_final}
\omega^{(m+1)}=\omega_i^{(m)}\cdot e^{\alpha_mI(y_i \neq G_m(x_i))}\cdot e^{-\beta_m}
\end{equation}
where $\alpha_m=2\beta_m$ and equation \ref{en:weight_final} is equivalent to algorithm \ref{alg:AdaBoost} line 6,  since the $e^{-\beta_m}$ do not change with i and multiply all the weights. So if we normalize the weights,  this term will have no impact. 


\section{Random forest}
\cite{breiman2001random} presents random forest method which connects bagging algorithm with random variable selection method. In his paper,  he shows that random forest model is competitive compared with arcing and boosting techniques. Besides,  random forest focus on variance reduction as well as decreasing bias. \\

\begin{algorithm}[H]
	\caption{Random forest,  \cite{breiman2001random}}\label{alg:random_forest}
	\nl \For{b=1 to B}{
		{(a) Draw a bootstrap sample $Z^*$ of size N from the training data.\\}
		{(b) Grow a random-forest tree $T_b$ to the bootstrapped data,  by re-
			cursively repeating the following steps for each terminal node of
			the tree,  until the minimum node size $n_{min}$ is reached.\\
			i. Select m variables at random from the p variables.\\
			ii. Pick the best variable/split-point among the m.\\
			iii. Split the node into two daughter nodes.\\
			
		}				         		    
	}
	\nl Output the ensemble of trees $\{T_b\}_1^B$ \\
	To make a prediction at a new point x:\\
	\nl Let $\hat{C}_b(x)$ be the class prediction of the both random forest tree. Then $\hat{C}_{rf}^B(x)$= majority vote $\{\hat{C}_b(x)\}_1^{B}$
\end{algorithm} 
\\

Algorithm \ref{alg:random_forest} shows the main procedure of random forest technique. In each step,  it uses the bootstrap method to generate a sample data set,  so the generated trees in each step are relatively de-correlated. Therefore the average of results in each step tends to reduce the bias of the model. Since an individual tree model is well known for its noisy property,  it is beneficial if we average different tree models. Build different identical distributed(i.d.) tress with low correlation can also reduce variance.  We can see that if we average $N$ i.i.d. random variables, each has a variance $\sigma^2$,  then the variance of the average is $\frac{\sigma^2}{N}$,  which will approach to zero when $N$ becomes infinity. In the case that the variables are not i.i.d but i.d,  the correlation between each pair is $\rho$,  then the variance of the average is:
\begin{equation}
\rho\sigma^2 +\frac{1-\rho}{N}\sigma^2
\end{equation} 

When N increases,  $\frac{1-\rho}{N}\sigma^2$ will decrease to zero. So average will also be beneficial to variance reduction in the i.d. case.  Moreover,  if through reducing the pairwise correlation $\rho$,  the first term $\rho\sigma^2$ can also be reduced. This can be achieved by random selecting of the input data. Besides, the tree models are highly non-linear,  so the bagging method during the process in the random forest  can improve to reduce the correlation $\rho$.  

When random forest method is used for solving classification problem,  it will draw the final solution by a majority vote among the result from each tree model. In a regression problem,  the final result of random forest regression model is simply the mean of results from each tree. In addition,  Breiman also makes some recommendation about how to choose the size $M$ to select the subset variables. For classification problem,  the minimum size of nodes is one and the default node size is $[\sqrt{p}]$. For regression problem,  the minimum size of nodes is five and the default node size is $[p/3]$,  where the notation "[ ]" represents the rounding function.